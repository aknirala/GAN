{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139bfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb349ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolConv(nn.Module):\n",
    "    \"\"\"\n",
    "    For conv: padding is same, stride is by default 1\n",
    "    \"\"\"\n",
    "    def __init__(self, name, in_ch, out_ch,\n",
    "                 filter_size, he_init=True, bias=True):\n",
    "        super(MeanPoolConv, self).__init__()\n",
    "        self.main = nn.Sequential()\n",
    "        avg_pool = nn.AvgPool2d(2)\n",
    "        conv = nn.Conv2d(in_ch, out_ch, filter_size, \n",
    "                         padding = (filter_size - 1)//2, bias=bias)\n",
    "        if he_init:\n",
    "            nn.init.kaiming_uniform_(conv.weight)\n",
    "        self.main.add_module(name + \"_avg_pool\", avg_pool)\n",
    "        self.main.add_module(name + \"_conv\", conv)\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)\n",
    "\n",
    "class ConvMeanPool(nn.Module):\n",
    "    \"\"\"\n",
    "    For conv: padding is same, stride is by default 1\n",
    "    \"\"\"\n",
    "    def __init__(self, name, in_ch, out_ch,\n",
    "                 filter_size, he_init=True, bias=True):\n",
    "        super(ConvMeanPool, self).__init__()\n",
    "        self.main = nn.Sequential()\n",
    "        conv = nn.Conv2d(in_ch, out_ch, filter_size, \n",
    "                         padding = (filter_size - 1)//2, bias=bias)\n",
    "        if he_init:\n",
    "            nn.init.kaiming_uniform_(conv.weight)\n",
    "        avg_pool = nn.AvgPool2d(2)\n",
    "        self.main.add_module(name + \"_conv\", conv)\n",
    "        self.main.add_module(name + \"_avg_pool\", avg_pool)\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)\n",
    "\n",
    "class UpsampleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    For conv: There is no padding here, stride is by default 1\n",
    "    \"\"\"\n",
    "    def __init__(self, name, in_ch, out_ch,\n",
    "                 filter_size, he_init=True, bias=True):\n",
    "        super(UpsampleConv, self).__init__()\n",
    "        self.main = nn.Sequential()\n",
    "        up_sample = nn.UpsamplingNearest2d(scale_factor = 2)\n",
    "        conv = nn.Conv2d(in_ch, out_ch, filter_size, \n",
    "                         padding = (filter_size - 1)//2, bias=bias)\n",
    "        if he_init:\n",
    "            nn.init.kaiming_uniform_(conv.weight)\n",
    "        self.main.add_module(name + \"_up_sample\", up_sample)\n",
    "        self.main.add_module(name + \"_conv\", conv)\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0796e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block Down\n",
    "class ResidualBlockDown(nn.Module):\n",
    "    def __init__(self, name, in_ch, out_ch, size, k_size = 3, stride=1, \n",
    "                 l_norm = True):\n",
    "        \"\"\"\n",
    "        resample = down means ResidualBlock is being used in discriminator\n",
    "        So we will use LayerNorm instead of batch_norm in discriminator.\n",
    "        \n",
    "        Here is what is happening (for down):\n",
    "        i/p --> || Normalize, ReLU, Conv1 (no channel change) || \n",
    "         |    --> Normalize, ReLU, ConvMeanPool (Sz half & channel change)\n",
    "         |_=> MeanPoolConv (Channel chane & sz half)\n",
    "        \n",
    "        \"\"\"\n",
    "        super(ResidualBlockDown, self).__init__()\n",
    "        self.s_cut = MeanPoolConv(name + \"_down_mpc\", in_ch, out_ch, k_size)\n",
    "        if l_norm:\n",
    "            self.nmlz1 = nn.LayerNorm([in_ch, size, size])\n",
    "            self.nmlz2 = nn.LayerNorm([in_ch, size, size])\n",
    "        else:\n",
    "            #We would use batch norm\n",
    "            self.nmlz1 = nn.BatchNorm2d(in_ch)\n",
    "            self.nmlz2 = nn.BatchNorm2d(in_ch)\n",
    "        #\n",
    "        #Now we would reach the same dimension as reached by \n",
    "        #      self.shortcut via two convolutinal loops.\n",
    "        #  self.shortcut would reach: [B, O_Ch, W//2, W//2]\n",
    "        #No channel change\n",
    "        self.R1 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_ch, in_ch, k_size,\n",
    "                              padding = (k_size - 1)//2, bias=False)\n",
    "        self.R2 = nn.ReLU()\n",
    "        self.conv2 = ConvMeanPool(name + \"_down_cmp\", in_ch, out_ch, k_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s_cut = self.s_cut(x)\n",
    "        out = self.R1(self.nmlz1(x))\n",
    "        out = self.conv1(out)\n",
    "        #out = self.R2(out)\n",
    "        out = self.R2(self.nmlz2(out))\n",
    "        out = self.conv2(out)\n",
    "        return out + s_cut\n",
    "\n",
    "# Residual block Up\n",
    "class ResidualBlockUp(nn.Module):\n",
    "    def __init__(self, name, in_ch, out_ch, k_size = 3, stride=1):\n",
    "        super(ResidualBlockUp, self).__init__()\n",
    "        self.s_cut = UpsampleConv(name + \"_up_sample_s_cut\", \n",
    "                                  in_ch, out_ch, k_size)\n",
    "        self.nmlz1 = nn.BatchNorm2d(in_ch)\n",
    "        self.nmlz2 = nn.BatchNorm2d(out_ch)\n",
    "        #\n",
    "        #Now we would reach the same dimension as reached by \n",
    "        #      self.shortcut via two convolutinal loops.\n",
    "        #  self.shortcut would reach: [B, O_Ch, W//2, W//2]\n",
    "        #No channel change\n",
    "        self.R1 = nn.ReLU()\n",
    "        self.conv1 = UpsampleConv(name + \"_up_sample_conv1_\", \n",
    "                                  in_ch, out_ch, k_size, bias=False)\n",
    "        self.R2 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, k_size,\n",
    "                              padding = (k_size - 1)//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s_cut = self.s_cut(x)\n",
    "        out = self.R1(self.nmlz1(x))\n",
    "        out = self.conv1(out)\n",
    "        out = self.R2(self.nmlz2(out))\n",
    "        out = self.conv2(out)\n",
    "        return out + s_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e9ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodGenerator(nn.Module):\n",
    "    def __init__(self, nz = None):\n",
    "        super(GoodGenerator, self).__init__()\n",
    "        global opt\n",
    "        #.view would be used in forward\n",
    "        \n",
    "        self.L = nn.Linear(opt.nz if nz is None else nz, 4*4*8*opt.dim)\n",
    "        self.R1 = ResidualBlockUp('G_R1', 8*opt.dim, 8*opt.dim)\n",
    "        self.R2 = ResidualBlockUp('G_R2', 8*opt.dim, 4*opt.dim)\n",
    "        self.R3 = ResidualBlockUp('G_R3', 4*opt.dim, 2*opt.dim)\n",
    "        self.R4 = ResidualBlockUp('G_R4', 2*opt.dim, opt.dim)\n",
    "        self.norm = nn.BatchNorm2d(opt.dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        #Below, we don't need to use formula for padding as kernel size \n",
    "        #    is being decided over here.\n",
    "        self.conv2rgb = nn.Conv2d(opt.dim, 3, kernel_size=3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        global opt\n",
    "        out = self.L(x).view(-1, 8*opt.dim, 4, 4)\n",
    "        out = self.R1(out)\n",
    "        out = self.R2(out)\n",
    "        out = self.R3(out)\n",
    "        out = self.R4(out)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2rgb(out)\n",
    "        return self.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e1e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoodDiscriminator, self).__init__()\n",
    "        global opt\n",
    "        self.rgb2conv = nn.Conv2d(3, opt.dim, kernel_size=3, padding=1)\n",
    "        self.R1 = ResidualBlockDown('D_R1', opt.dim, 2*opt.dim, 64)\n",
    "        self.R2 = ResidualBlockDown('D_R2', 2*opt.dim, 4*opt.dim, 32)\n",
    "        self.R3 = ResidualBlockDown('D_R3', 4*opt.dim, 8*opt.dim, 16)\n",
    "        self.R4 = ResidualBlockDown('D_R4', 8*opt.dim, 8*opt.dim, 8)\n",
    "        self.L = nn.Linear(4*4*8*opt.dim, 1)\n",
    "    def forward(self, x):\n",
    "        global opt\n",
    "        out = self.rgb2conv(x)\n",
    "        out = self.R1(out)\n",
    "        out = self.R2(out)\n",
    "        out = self.R3(out)\n",
    "        out = self.R4(out)\n",
    "        #print(out.shape, opt.dim)\n",
    "        return self.L(out.view(-1, 4*4*8*opt.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343c3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    \"\"\"\n",
    "    This is voodoo of gradient penalty which makes everything glitter.\n",
    "    \"\"\"\n",
    "    global opt\n",
    "    alpha = torch.rand(real_data.shape[0], 1) #This is b/w 0 and 1\n",
    "    #alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.expand(real_data.shape[0], \n",
    "                         real_data.nelement()//real_data.shape[0]).contiguous().view(\n",
    "                        *real_data.shape)\n",
    "    alpha = alpha.cuda(gpu) if cuda else alpha\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    if cuda:\n",
    "        interpolates = interpolates.cuda(gpu)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "    gradients = autograd.grad(outputs      = disc_interpolates, \n",
    "                              inputs       = interpolates,\n",
    "                              grad_outputs = torch.ones(\n",
    "                                                  disc_interpolates.size()\n",
    "                                              ).cuda() if cuda else torch.ones(\n",
    "                                                  disc_interpolates.size()),\n",
    "                              create_graph = True,\n",
    "                              retain_graph = True, \n",
    "                              #only_inputs  = True #This is True by default.\n",
    "                             )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * opt.lambda_value\n",
    "    #I believe .mean() is not needed\n",
    "    #gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2) * LAMBDA\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fde0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_WPG_iters(reals, noises, dgs, losses):\n",
    "    \"\"\"\n",
    "    This will perform the criitc iterations while doing WGAN_PG training. and then G iter\n",
    "    reals: an array of real data\n",
    "    noise: an array of noise. It is 1 more than reals in size (for G iteration)\n",
    "    dgs: an array containing [G, D, optG, optD], in order\n",
    "    losses: an array fo rall the losses expected while doing WGAN_PG training\n",
    "            so, the array would be in order: [R_COST, F_COST, GP, W_DIST, D_COST, G_COST]\n",
    "            \n",
    "    \"\"\"\n",
    "    #Static part are made global\n",
    "    global one, mone\n",
    "    G, D, optG, optD = dgs\n",
    "    R_COST, F_COST, GP, W_DIST, D_COST, G_COST = losses\n",
    "    for p in D.parameters():\n",
    "        p.requires_grad = True\n",
    "    for c_iters in range(len(reals)):\n",
    "        D.zero_grad()\n",
    "        #\n",
    "        #Train with reals\n",
    "        data = reals[c_iters].cuda(gpu)\n",
    "        D_real = D(data.float())\n",
    "        D_real = D_real.mean().reshape(1)\n",
    "        D_real.backward(one)\n",
    "        #\n",
    "        #Train with fakes\n",
    "        noise = noises[c_iters].cuda(gpu)\n",
    "        with torch.no_grad():\n",
    "            fakes = G(noise).data\n",
    "        D_fake = D(fakes)\n",
    "        D_fake = D_fake.mean().reshape(1)\n",
    "        D_fake.backward(mone)\n",
    "        #\n",
    "        # train with gradient penalty\n",
    "        gradient_penalty = calc_gradient_penalty(D, data.data, fakes.data)\n",
    "        gradient_penalty.backward()\n",
    "        optD.step()\n",
    "        #--------------Now append losses------------\n",
    "        \n",
    "        R_COST.append(float(D_real))\n",
    "        F_COST.append(float(D_fake))\n",
    "        GP.append(float(gradient_penalty))\n",
    "        Wasserstein_D = D_real - D_fake\n",
    "        W_DIST.append(float(Wasserstein_D))\n",
    "        D_cost = D_fake - D_real + gradient_penalty\n",
    "        D_COST.append(float(D_cost))\n",
    "    #\n",
    "    ############################\n",
    "    # (2) Update G network\n",
    "    ###########################\n",
    "    for p in D.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    G.zero_grad()\n",
    "    #\n",
    "    noise = noises[-1].cuda(gpu)\n",
    "    fakes = G(noise)\n",
    "    G_cost = D(fakes)\n",
    "    G_cost = G_cost.mean().reshape(1)\n",
    "    G_cost.backward(one)\n",
    "    optG.step()\n",
    "    G_COST.append(float(G_cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gan_38] *",
   "language": "python",
   "name": "conda-env-gan_38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
