{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b899a553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main', '--niter', '10000', '--loadPreGPath', 'preG.mdl']\n",
      "Namespace(adam=False, batchSize=64, beta1=0.5, beta2=0.9, clamp_lower=-0.01, clamp_upper=0.01, critic_iters=5, dataroot='/home/aknirala/data/lsun', dataset='diverse_clocks', dim=64, fineSize=64, getZ=False, hm_gen_size=30, lambda_value=10.0, loadPreGPath='preG.mdl', loadSize=96, lrD=0.0002, lrG=0.0002, nEpochs=5, nc=3, ndf=64, newGetZ=False, ngf=64, ngpu=1, niter=10000, noCuda=False, noise='normal', notLog=False, nz=100, opFolder='', resumeFldr='', seed=1, useLN=False, workers=8)\n",
      "OP folder won't be created as we are in jupyter notebook and logging is off\n",
      "Arguments are:  Namespace(adam=False, batchSize=64, beta1=0.5, beta2=0.9, clamp_lower=-0.01, clamp_upper=0.01, critic_iters=5, dataroot='/home/aknirala/data/lsun', dataset='diverse_clocks', dim=64, fineSize=64, getZ=False, hm_gen_size=30, lambda_value=10.0, loadPreGPath='preG.mdl', loadSize=96, lrD=0.0002, lrG=0.0002, nEpochs=5, nc=3, ndf=64, newGetZ=False, ngf=64, ngpu=1, niter=10000, noCuda=False, noise='normal', notLog=False, nz=100, opFolder='', resumeFldr='', seed=1, useLN=False, workers=8)\n",
      "Len of iterations per epochs:  15625\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "There are two variants to try: in one h and m encoding is first converted to noise...\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "%run common/imp_and_args1.ipynb\n",
    "if ntbk:\n",
    "    sys.argv = ['main',\n",
    "                '--niter', '10000', \"--loadPreGPath\", \"preG.mdl\" ]#, ]#, \"--critic_iters\", \"5\"] , '--useLN'\n",
    "print(sys.argv)\n",
    "#Now add some parser argument here, commented examples follow\n",
    "#Boolean, action is what it will be set to when passed\n",
    "#parser.add_argument('--notLog', action='store_true', help='if passed, no logging will happen. Logging is off for jupyter anyways.;')\n",
    "#String\n",
    "#parser.add_argument('--opFolder', default='', help='Override the logging folder.')\n",
    "#int\n",
    "#parser.add_argument('--seed', type=int, default=1, help='Seed for pyTorcj.')\n",
    "#and float\n",
    "#parser.add_argument('--clamp_lower', type=float, default=-0.01)\n",
    "\n",
    "\n",
    "parser.add_argument('--loadPreGPath', default='', \n",
    "                    help=\"if passed, PreG would be loaded from this path\")\n",
    "\n",
    "\n",
    "parser.add_argument('--newGetZ', action='store_true', \n",
    "                    help=\"if passed, GetZ won't be created form D, \"+\n",
    "                    \" but will be created from scratch. And Batch \"+\n",
    "                    \" Normalization would be used.\")\n",
    "\n",
    "parser.add_argument('--useLN', action='store_true', \n",
    "                    help=\"if passed, Layer Norm would be used, else batch norm.\")\n",
    "parser.add_argument('--getZ', action='store_true', \n",
    "                    help=\"if passed, then inverse mapping will get z, else it will get L.\")\n",
    "\n",
    "%run common/args_and_rest.ipynb\n",
    "g_nz = opt.hm_gen_size + opt.nz\n",
    "fixed_noise = torch.randn(opt.batchSize, g_nz)#, 1, 1)\n",
    "if cuda:\n",
    "    fixed_noise = fixed_noise.cuda(gpu)\n",
    "%run common/Utils.ipynb\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb620151",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = same_style_clocks(randSeed=1, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]))\n",
    "dload2 = torch.utils.data.DataLoader(dset2, batch_size=opt.batchSize,\n",
    "                                            shuffle=True, num_workers=int(opt.workers))\n",
    "same_style_iter = iter(dload2)\n",
    "data = same_style_iter.next()\n",
    "clocks = data[0]#.reshape(-1, 3, 64, 64)\n",
    "plt.imshow(vutils.make_grid(\n",
    "     clocks[:16],\n",
    "    padding = 2, nrow = 4, normalize = True\n",
    ").numpy().transpose([1, 2, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run freezed/WGAN_PG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreGen(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreGen, self).__init__()\n",
    "        global opt\n",
    "        self.main = nn.Sequential()\n",
    "        L1 = nn.Linear(72, 1024)\n",
    "        relu1 = nn.ReLU()\n",
    "        L2 = nn.Linear(1024, 1024)\n",
    "        relu2 = nn.ReLU()\n",
    "        L3 = nn.Linear(1024, opt.hm_gen_size)\n",
    "        self.main.add_module(\"PreGen_L1\", L1)\n",
    "        self.main.add_module(\"PreGen_relu1\", relu1)\n",
    "        self.main.add_module(\"PreGen_L2\", L2)\n",
    "        self.main.add_module(\"PreGen_relu2\", relu2)\n",
    "        self.main.add_module(\"PreGen_L3\", L3)\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3afeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreDis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreDis, self).__init__()\n",
    "        global opt\n",
    "        self.main = nn.Sequential()\n",
    "        L1 = nn.Linear(opt.hm_gen_size, 1024)\n",
    "        relu1 = nn.ReLU()\n",
    "        L2 = nn.Linear(1024, 1024)\n",
    "        relu2 = nn.ReLU()\n",
    "        L3 = nn.Linear(1024, 1)\n",
    "        self.main.add_module(\"PreGen_L1\", L1)\n",
    "        self.main.add_module(\"PreGen_relu1\", relu1)\n",
    "        self.main.add_module(\"PreGen_L2\", L2)\n",
    "        self.main.add_module(\"PreGen_relu2\", relu2)\n",
    "        self.main.add_module(\"PreGen_L3\", L3)\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We would use WGAN_PG to train this\n",
    "preG = PreGen()\n",
    "optPreG = optim.Adam(preG.parameters(), lr=opt.lrD, betas=(opt.beta1, opt.beta2))\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if cuda:\n",
    "    one  = one.cuda(gpu)\n",
    "    mone = mone.cuda(gpu)\n",
    "    preG = preG.cuda(gpu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38057633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomHM():\n",
    "    global opt\n",
    "    H = []\n",
    "    M = []\n",
    "    h_eye = torch.eye(12)\n",
    "    m_eye = torch.eye(60)\n",
    "    for _ in range(opt.batchSize):\n",
    "        H.append(np.random.randint(12))\n",
    "        M.append(np.random.randint(60))\n",
    "    H = h_eye[H].clone().cuda(gpu)\n",
    "    M = m_eye[M].clone().cuda(gpu)\n",
    "    hm = torch.cat([H, M], axis=1).cuda(gpu)\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.loadPreGPath != '':\n",
    "    preG_chkpt = torch.load(opt.loadPreGPath)\n",
    "    preG.load_state_dict(preG_chkpt)\n",
    "    preG = preG.cuda()\n",
    "    preG.eval()\n",
    "else:\n",
    "    D_real = 1\n",
    "    D_fake = -1\n",
    "    gradient_penalty = opt.lambda_value\n",
    "    G_cost = 1\n",
    "    iters = -1\n",
    "    R_COST = []\n",
    "    F_COST = []\n",
    "    W_DIST = []\n",
    "    D_COST = []\n",
    "    G_COST = []\n",
    "    GP = []\n",
    "    preD = PreDis().cuda(gpu)\n",
    "    optPreD = optim.Adam(preD.parameters(), lr=opt.lrD, betas=(opt.beta1, opt.beta2))\n",
    "\n",
    "    for epoch in range(2):\n",
    "        plog(\"Epoch: \", epoch, \" of \", opt.nEpochs, \" iters: \", iters)\n",
    "        i = 0\n",
    "        pbar = tqdm(range(10000))\n",
    "        for i in pbar:\n",
    "            pbar.set_description( (\" D_real {:.3f}, D_fake {:.3f}, \"\\\n",
    "                +\"gradient_penalty {:.3f}, G_cost {:.3f},  {}\").format(\n",
    "                float(D_real), float(D_fake), \n",
    "                float(gradient_penalty), float(G_cost), iters))\n",
    "            iters += 1\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in preD.parameters():  # reset requires_grad\n",
    "                p.requires_grad = True  # they are set to False below in netG update\n",
    "            #\n",
    "            for iter_d in range(opt.critic_iters):\n",
    "                optPreD.zero_grad()\n",
    "                # train with real ####################\n",
    "                data = torch.randn(opt.batchSize, opt.hm_gen_size).cuda(gpu)\n",
    "                D_real = preD(data.float())\n",
    "                D_real = D_real.mean().reshape(1)\n",
    "                if iters == 0 and iter_d == 0: plog(\"D_real.shape: \",D_real.shape)\n",
    "                D_real.backward(one)\n",
    "                #\n",
    "                # train with fakes ##########################\n",
    "                hm = getRandomHM()\n",
    "                with torch.no_grad():\n",
    "                    fakes = preG(hm).data\n",
    "                D_fake = preD(fakes)\n",
    "                D_fake = D_fake.mean().reshape(1)\n",
    "                if iters == 0 and iter_d == 0: plog(\"D_fake.shape: \",D_fake.shape)\n",
    "                D_fake.backward(mone)\n",
    "                #\n",
    "                # train with gradient penalty\n",
    "                gradient_penalty = calc_gradient_penalty(preD, data.data, \n",
    "                                                         fakes.data)\n",
    "                if iters == 0 and iter_d == 0: plog(\"gradient_penalty.shape: \",gradient_penalty.shape)\n",
    "                gradient_penalty.backward()\n",
    "                GP.append(float(gradient_penalty))\n",
    "                R_COST.append(float(D_real))\n",
    "                F_COST.append(float(D_fake))\n",
    "                Wasserstein_D = D_real - D_fake\n",
    "                W_DIST.append(float(Wasserstein_D))\n",
    "                D_cost = D_fake - D_real + gradient_penalty\n",
    "                #\n",
    "                optPreD.step()\n",
    "                D_COST.append(float(D_cost))\n",
    "            #\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "            for p in preD.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "            preG.zero_grad()\n",
    "            #\n",
    "            hm = getRandomHM()\n",
    "            fakes =preG(hm)\n",
    "            G_cost = preD(fakes)\n",
    "            G_cost = G_cost.mean().reshape(1)\n",
    "            G_cost.backward(one)\n",
    "            optPreG.step()\n",
    "            G_COST.append(-float(G_cost))\n",
    "            #\n",
    "            if iters%5000 == 0 or iters in disp_itrs:\n",
    "                generate_image([preD, preG, optPreD, optPreG],\n",
    "                               iters, data.data, \n",
    "                               loss_dict1 = {\"R_COST\":R_COST,\n",
    "                                            \"F_COST\":F_COST,\n",
    "                                            \"W_DIST\":W_DIST,\n",
    "                                            #\"D_COST\":D_COST,\n",
    "                                            \"GP\":GP}, \n",
    "                               loss_dict2 = {\"G_COST\":G_COST,\n",
    "                                            },\n",
    "                               fakes = fakes.data,\n",
    "                               imgType = 'hist'\n",
    "                              )#, fakes.data)\n",
    "            #if ntbk and iters > 500:\n",
    "            #    break\n",
    "        pbar.close()\n",
    "    torch.save(preG.state_dict(), \"preG.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877de914",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.randn(5)\n",
    "n.expand( (2, 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_noise = torch.randn(opt.nz)\n",
    "s_style_noise = s_noise.expand((opt.batchSize, opt.nz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0c98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_noise = torch.randn(opt.nz)\n",
    "s_style_noise = s_noise.expand((opt.batchSize, opt.nz))\n",
    "mse = nn.MSELoss()\n",
    "if cuda:\n",
    "    s_style_noise = s_style_noise.cuda(gpu)\n",
    "    mse = mse.cuda(gpu)\n",
    "fixed_noise[1][opt.hm_gen_size:] = s_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_r_state = np.random.get_state()\n",
    "np.random.seed(2)\n",
    "#This is being done coz of multi-threading (minimize the effect of drastic change)\n",
    "same_style_dset = same_style_clocks(randSeed=2, transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Resize(64),\n",
    "                                transforms.CenterCrop(64),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "same_style_dloader = torch.utils.data.DataLoader(\n",
    "    same_style_dset, batch_size=opt.batchSize,\n",
    "    shuffle=True, num_workers=int(opt.workers))\n",
    "np.random.set_state(org_r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = GoodGenerator(g_nz)\n",
    "plog(G)\n",
    "D = GoodDiscriminator()\n",
    "plog(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9589ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now declare the variables!!\n",
    "optD = optim.Adam(D.parameters(), lr=opt.lrD, betas=(opt.beta1, opt.beta2))\n",
    "optG = optim.Adam(G.parameters(), lr=opt.lrG, betas=(opt.beta1, opt.beta2))\n",
    "optGDec = optim.Adam(G.parameters(), lr=opt.lrG, betas=(opt.beta1, opt.beta2))\n",
    "R_COST = []\n",
    "F_COST = []\n",
    "W_DIST = []\n",
    "D_COST = []\n",
    "G_COST = []\n",
    "GP = []\n",
    "\n",
    "iters = -1\n",
    "try:\n",
    "    loadWeights(opt.resumeFldr, loss_dict={\n",
    "                        \"R_COST.pkl\": R_COST,\n",
    "                        \"F_COST.pkl\": F_COST,\n",
    "                        \"W_DIST.pkl\": W_DIST,\n",
    "                        \"D_COST.pkl\": D_COST,\n",
    "                        \"G_COST.pkl\": G_COST,\n",
    "                    })\n",
    "except:\n",
    "    plog(\"Encountered error while loading weights!!\")\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if cuda:\n",
    "    one = one.cuda(gpu)\n",
    "    mone = mone.cuda(gpu)\n",
    "    D = D.cuda(gpu)\n",
    "    G = G.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4022cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c0c97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_len = len(dataloader)\n",
    "D_real = 1\n",
    "D_fake = -1\n",
    "gradient_penalty = opt.lambda_value\n",
    "G_cost = 1\n",
    "DEC_LS = []\n",
    "loss = 1\n",
    "for epoch in range(opt.nEpochs):\n",
    "    plog(\"Epoch: \", epoch, \" of \", opt.nEpochs, \" iters: \", iters)\n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    #Now, let's train it\n",
    "    #This is being doen due to multi-threading\n",
    "    org_r_state = np.random.get_state()\n",
    "    np.random.seed(2)\n",
    "    s_style_iter = iter(same_style_dloader)\n",
    "    np.random.set_state(org_r_state)\n",
    "    \n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(data_iter)//opt.critic_iters)\n",
    "    #pbar = tqdm(range(len(data_iter)//opt.critic_iters))\n",
    "    while i < data_len - opt.critic_iters:\n",
    "        pbar.set_description( (\" D_real {:.3f}, D_fake {:.3f}, \"\\\n",
    "        +  \"gradient_penalty {:.3f}, G_cost {:.3f}, DEC_loss{:.3f},  {}\").format(\n",
    "            float(D_real), float(D_fake), \n",
    "            float(gradient_penalty), float(G_cost), float(loss), iters))\n",
    "        #pbar.update(1)\n",
    "        iters += 1\n",
    "        reals = []\n",
    "        noises = []\n",
    "        for iter_d in range(opt.critic_iters):\n",
    "            reals.append(next(data_iter)[0])\n",
    "            noises.append(torch.randn(opt.batchSize, g_nz))\n",
    "        noises.append(torch.randn(opt.batchSize, g_nz))\n",
    "        do_WPG_iters(reals, noises, \n",
    "                     [G, D, optG, optD], \n",
    "                     [R_COST, F_COST, GP, W_DIST, D_COST, G_COST])\n",
    "        #\n",
    "        ############################################\n",
    "        # Now one iteration of Decoder training\n",
    "        ###########################################\n",
    "        #This is being doen due to multi-threading\n",
    "        org_r_state = np.random.get_state()\n",
    "        np.random.seed(2)\n",
    "        #\n",
    "        dec_data, H, M = next(s_style_iter)\n",
    "        np.random.set_state(org_r_state)\n",
    "        HM = torch.cat([H, M], axis=1).cuda(gpu)\n",
    "        with torch.no_grad():\n",
    "            hm_to_g = preG(HM)\n",
    "            #Add a little noise\n",
    "            hm_plus_noise = hm_to_g + torch.randn(hm_to_g.shape).cuda(gpu)/1000\n",
    "            inp_g = torch.cat([hm_plus_noise, s_style_noise], axis = 1)\n",
    "        #\n",
    "        optGDec.zero_grad()\n",
    "        dec_op = G(inp_g)\n",
    "        loss = mse(dec_data.cuda(gpu), dec_op)\n",
    "        loss.backward()\n",
    "        optGDec.step()\n",
    "        DEC_LS.append(float(loss))\n",
    "        if iters%5000 == 0 or iters in disp_itrs:\n",
    "            disp_data = []\n",
    "            #Fixed time and style  #fixed noise 1 has same style!!! \n",
    "            #Clocks 0 adn 1 will have same time as that shown in real\n",
    "            #Further clock 1 should have same style as well\n",
    "            for d_i in range(2):\n",
    "                disp_data.append(dec_data[d_i].cuda(gpu).data)\n",
    "                fixed_noise[d_i][:opt.hm_gen_size] = hm_to_g[d_i].data\n",
    "            #These two will be fresh as generated\n",
    "            for d_i in range(2): disp_data.append(reals[-1][d_i].cuda(gpu).data)\n",
    "            oToPickle = {}\n",
    "            oToPickle[\"fixed_noise\"] = fixed_noise\n",
    "            #preG, optPreG\n",
    "            oToPickle[\"optGDec\"] = optGDec.state_dict()\n",
    "            generate_image([D, G, optD, optG],\n",
    "                           iters, torch.stack(disp_data), \n",
    "                           loss_dict1 = {\"R_COST\":R_COST,\n",
    "                                        \"F_COST\":F_COST,\n",
    "                                        \"W_DIST\":W_DIST,\n",
    "                                        #\"D_COST\":D_COST,\n",
    "                                        \"GP\":GP}, \n",
    "                           loss_dict2 = {\"G_COST\":G_COST,\n",
    "                                        \"DEC_LS\":DEC_LS,\n",
    "                                        },\n",
    "                           oToPickle = oToPickle)#, fakes.data)\n",
    "        #if ntbk and iters > 500:\n",
    "        #    break\n",
    "    pbar.close()\n",
    "#with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a849f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gan_38] *",
   "language": "python",
   "name": "conda-env-gan_38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
